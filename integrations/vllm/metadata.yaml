id: vllm
short_name: vLLM
display_name: vLLM
description: |
  vLLM is a highly optimized open source LLM serving framework that can increase serving throughput on GPUs. The integration collects metrics related to throughput, cache usage and inference latency.
